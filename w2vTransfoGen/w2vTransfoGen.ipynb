{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e77164c",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "188b023f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sah/miniconda3/envs/nlp-pytorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7f583f",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc24f135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title     string[python]\n",
       "text      string[python]\n",
       "author    string[python]\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"../classification/data/en_poems.parquet\")\n",
    "df = df.astype({\"title\": \"string\", \"text\": \"string\", \"author\": \"string\"})\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20858584",
   "metadata": {},
   "source": [
    "## PREPROCESS THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5a73da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Song for an Unwritten Play.</td>\n",
       "      <td>The moon's a drowsy fool to-night,\n",
       "Wrapped in ...</td>\n",
       "      <td>Shanks, Edward</td>\n",
       "      <td>[the, moons, a, drowsy, fool, tonight, wrapped...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Cup.</td>\n",
       "      <td>As a hot traveller\n",
       "Going through stones and sa...</td>\n",
       "      <td>Shanks, Edward</td>\n",
       "      <td>[as, a, hot, traveller, going, through, stones...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Rhymeless Song.</td>\n",
       "      <td>Rhyme with its jingle still betrays\n",
       "The song t...</td>\n",
       "      <td>Shanks, Edward</td>\n",
       "      <td>[rhyme, with, its, jingle, still, betrays, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meadow and Orchard.</td>\n",
       "      <td>My heart is like a meadow,\n",
       "Where clouds go ove...</td>\n",
       "      <td>Shanks, Edward</td>\n",
       "      <td>[my, heart, is, like, a, meadow, where, clouds...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who thinks that he possesses.</td>\n",
       "      <td>Who thinks that he possesses\n",
       "His mistress with...</td>\n",
       "      <td>Shanks, Edward</td>\n",
       "      <td>[who, thinks, that, he, possesses, his, mistre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41801</th>\n",
       "      <td>XXVIII.</td>\n",
       "      <td>Sole Maker of the Worlds! They lay\n",
       "A barren bl...</td>\n",
       "      <td>De Vere, Aubrey</td>\n",
       "      <td>[sole, maker, of, the, worlds, they, lay, a, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41802</th>\n",
       "      <td>XXIX.</td>\n",
       "      <td>When from beneath the Almighty Hand\n",
       "The suns a...</td>\n",
       "      <td>De Vere, Aubrey</td>\n",
       "      <td>[when, from, beneath, the, almighty, hand, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41803</th>\n",
       "      <td>XXX.</td>\n",
       "      <td>A woman “clothed with the sun,”\n",
       "Yet fleeing fr...</td>\n",
       "      <td>De Vere, Aubrey</td>\n",
       "      <td>[a, woman, clothed, with, the, sun, yet, fleei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41804</th>\n",
       "      <td>XXXI.</td>\n",
       "      <td>No ray of all their silken sheen\n",
       "The leaves fi...</td>\n",
       "      <td>De Vere, Aubrey</td>\n",
       "      <td>[no, ray, of, all, their, silken, sheen, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41805</th>\n",
       "      <td>EPILOGUE.</td>\n",
       "      <td>Epilogue\n",
       "Regent of Change, thou waning Moon,\n",
       "W...</td>\n",
       "      <td>De Vere, Aubrey</td>\n",
       "      <td>[epilogue, regent, of, change, thou, waning, m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41806 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title  \\\n",
       "0        Song for an Unwritten Play.   \n",
       "1                           The Cup.   \n",
       "2                  A Rhymeless Song.   \n",
       "3                Meadow and Orchard.   \n",
       "4      Who thinks that he possesses.   \n",
       "...                              ...   \n",
       "41801                        XXVIII.   \n",
       "41802                          XXIX.   \n",
       "41803                           XXX.   \n",
       "41804                          XXXI.   \n",
       "41805                      EPILOGUE.   \n",
       "\n",
       "                                                    text           author  \\\n",
       "0      The moon's a drowsy fool to-night,\n",
       "Wrapped in ...   Shanks, Edward   \n",
       "1      As a hot traveller\n",
       "Going through stones and sa...   Shanks, Edward   \n",
       "2      Rhyme with its jingle still betrays\n",
       "The song t...   Shanks, Edward   \n",
       "3      My heart is like a meadow,\n",
       "Where clouds go ove...   Shanks, Edward   \n",
       "4      Who thinks that he possesses\n",
       "His mistress with...   Shanks, Edward   \n",
       "...                                                  ...              ...   \n",
       "41801  Sole Maker of the Worlds! They lay\n",
       "A barren bl...  De Vere, Aubrey   \n",
       "41802  When from beneath the Almighty Hand\n",
       "The suns a...  De Vere, Aubrey   \n",
       "41803  A woman “clothed with the sun,”\n",
       "Yet fleeing fr...  De Vere, Aubrey   \n",
       "41804  No ray of all their silken sheen\n",
       "The leaves fi...  De Vere, Aubrey   \n",
       "41805  Epilogue\n",
       "Regent of Change, thou waning Moon,\n",
       "W...  De Vere, Aubrey   \n",
       "\n",
       "                                                  tokens  \n",
       "0      [the, moons, a, drowsy, fool, tonight, wrapped...  \n",
       "1      [as, a, hot, traveller, going, through, stones...  \n",
       "2      [rhyme, with, its, jingle, still, betrays, the...  \n",
       "3      [my, heart, is, like, a, meadow, where, clouds...  \n",
       "4      [who, thinks, that, he, possesses, his, mistre...  \n",
       "...                                                  ...  \n",
       "41801  [sole, maker, of, the, worlds, they, lay, a, b...  \n",
       "41802  [when, from, beneath, the, almighty, hand, the...  \n",
       "41803  [a, woman, clothed, with, the, sun, yet, fleei...  \n",
       "41804  [no, ray, of, all, their, silken, sheen, the, ...  \n",
       "41805  [epilogue, regent, of, change, thou, waning, m...  \n",
       "\n",
       "[41806 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text: str) -> list[str]:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text) # remove punctuation\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "df[\"tokens\"] = df[\"text\"].apply(preprocess_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5104eb5",
   "metadata": {},
   "source": [
    "## CREATE THE DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1896bc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 39715/39715 [00:05<00:00, 6764.81 examples/s]\n",
      "Map: 100%|██████████| 2091/2091 [00:00<00:00, 6508.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(df[[\"text\"]], test_size=0.05, random_state=42)\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f3c1a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12410' max='12410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12410/12410 1:43:30, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.888800</td>\n",
       "      <td>3.826202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.735000</td>\n",
       "      <td>3.750368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.670400</td>\n",
       "      <td>3.711607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.597100</td>\n",
       "      <td>3.687578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.561200</td>\n",
       "      <td>3.671891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.516100</td>\n",
       "      <td>3.658936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.495700</td>\n",
       "      <td>3.653160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.470600</td>\n",
       "      <td>3.647950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.421600</td>\n",
       "      <td>3.646008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12410, training_loss=3.597428992100054, metrics={'train_runtime': 6211.034, 'train_samples_per_second': 63.943, 'train_steps_per_second': 1.998, 'total_flos': 2.5924023631872e+16, 'train_loss': 3.597428992100054, 'epoch': 9.992547834843908})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\"\"\"training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\"\"\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7bd1df",
   "metadata": {},
   "source": [
    "## SAVE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6188d353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./gpt2-poem-model/tokenizer_config.json',\n",
       " './gpt2-poem-model/special_tokens_map.json',\n",
       " './gpt2-poem-model/vocab.json',\n",
       " './gpt2-poem-model/merges.txt',\n",
       " './gpt2-poem-model/added_tokens.json',\n",
       " './gpt2-poem-model/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./gpt2-poem-model\")\n",
    "tokenizer.save_pretrained(\"./gpt2-poem-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ae5c1d",
   "metadata": {},
   "source": [
    "## GENERATE A POEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b18b7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It never ends,\n",
      "And never has been;\n",
      "For the first time, I am old,\n",
      "And in this world's past;\n",
      "The world's past.\n",
      "And when it stops,\n",
      "And a new thing comes in,\n",
      "I will lie down at my bedside,\n",
      "I will lie down at my bedside,\n",
      "And, with a sigh, I will turn and walk,\n",
      "And all the world shall know.\n",
      "Yes, I will lie down at my bedside\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"./gpt2-poem-model\", tokenizer=\"./gpt2-poem-model\")\n",
    "\n",
    "prompt = \"It never ends\"\n",
    "results = generator(prompt, max_length=100, num_return_sequences=1, do_sample=True, top_k=50, top_p=0.95)\n",
    "\n",
    "print(results[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05f7aec",
   "metadata": {},
   "source": [
    "## EVALUATION METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe0a7e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 214.04it/s]\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "prompts = [\n",
    "    \"It never ends\",\n",
    "    \"The moonlight dances\",\n",
    "    \"Darkness falls quickly\",\n",
    "    \"Beneath the willow tree\",\n",
    "    \"Whispers in the wind\",\n",
    "    \"I dreamed of fire\",\n",
    "    \"The silence grew louder\",\n",
    "    \"Stars fell like rain\",\n",
    "    \"Time forgets no one\",\n",
    "    \"A rose in winter\",\n",
    "    \"Shadows crawl at dawn\",\n",
    "    \"My heart is a lantern\",\n",
    "    \"Echoes of your name\",\n",
    "    \"Frozen in memory\",\n",
    "    \"We walked on glass\",\n",
    "    \"The sky swallowed the sun\",\n",
    "    \"Love fades to smoke\",\n",
    "    \"Buried beneath the snow\",\n",
    "    \"A storm without sound\",\n",
    "    \"Hope wears thin threads\"\n",
    "]\n",
    "\n",
    "all_poems = df[\"text\"].tolist()\n",
    "\n",
    "poem_embeddings = model.encode(all_poems, convert_to_tensor=True)\n",
    "\n",
    "best_refs = []\n",
    "\n",
    "for prompt in tqdm(prompts):\n",
    "    prompt_embedding = model.encode(prompt, convert_to_tensor=True)\n",
    "    similarities = util.cos_sim(prompt_embedding, poem_embeddings)[0]\n",
    "    best_index = similarities.argmax().item()\n",
    "    best_poem = all_poems[best_index]\n",
    "    best_refs.append(best_poem)\n",
    "    #print(f\"\\nPrompt: {prompt}\\nBest Reference Poem:\\n{best_poem}\\n{'-'*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a6f935c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores:\n",
      "rouge1: 0.1917\n",
      "rouge2: 0.0179\n",
      "rougeL: 0.1262\n",
      "rougeLsum: 0.1844\n",
      "BERTScore (averaged):\n",
      "Precision: 0.838\n",
      "Recall: 0.8277\n",
      "F1: 0.8328\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"./gpt2-poem-model\", tokenizer=\"./gpt2-poem-model\")\n",
    "\n",
    "generated_poems = [\n",
    "    generator(prompt, max_length=100, num_return_sequences=1, do_sample=True, top_k=50, top_p=0.95)[0][\"generated_text\"]\n",
    "    for prompt in prompts\n",
    "]\n",
    "\n",
    "# Load metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Evaluate all\n",
    "rouge_score = rouge.compute(predictions=generated_poems, references=best_refs)\n",
    "bert_score = bertscore.compute(predictions=generated_poems, references=best_refs, lang=\"en\")\n",
    "\n",
    "# Print results\n",
    "print(\"ROUGE Scores:\")\n",
    "for key, val in rouge_score.items():\n",
    "    print(f\"{key}: {round(val, 4)}\")\n",
    "\n",
    "print(\"BERTScore (averaged):\")\n",
    "print(\"Precision:\", round(sum(bert_score[\"precision\"]) / len(bert_score[\"precision\"]), 4))\n",
    "print(\"Recall:\", round(sum(bert_score[\"recall\"]) / len(bert_score[\"recall\"]), 4))\n",
    "print(\"F1:\", round(sum(bert_score[\"f1\"]) / len(bert_score[\"f1\"]), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69ae19a",
   "metadata": {},
   "source": [
    "## METRICS ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250c9f9b",
   "metadata": {},
   "source": [
    "The ROUGE scores being a bit low tell us that our generated poems are different in the exact word choice and structure form from the references. We believe it is not something to be worried about since poetry should be really free in terms of choices.\n",
    "\n",
    "On the other hand, for BERT, we believe that an F1 >0.83 is consistently strong, especially across the 20 different prompts we made. The model has a decent semantic alignement and it shows that it can produce meaningful and related poetry. It is not just grammatically coherent but also thematically and semantically grounded.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
