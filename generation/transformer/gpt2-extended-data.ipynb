{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e77164c",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "188b023f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sah/miniconda3/envs/nlp-pytorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7f583f",
   "metadata": {},
   "source": [
    "## LOAD DATA AND TRANSLATED DATA, THEN MERGE BOTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc24f135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title       string[python]\n",
       "text        string[python]\n",
       "author      string[python]\n",
       "creation            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_parquet(\"../../data/en_poems.parquet\")\n",
    "df2 = pd.read_parquet(\"../../data/de_translated_en.parquet\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "df = df.astype({\"title\": \"string\", \"text\": \"string\", \"author\": \"string\"})\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e9c0fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79959"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5104eb5",
   "metadata": {},
   "source": [
    "## CREATE THE DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1896bc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 75961/75961 [00:08<00:00, 8462.78 examples/s]\n",
      "Map: 100%|██████████| 3998/3998 [00:00<00:00, 8985.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(df[[\"text\"]], test_size=0.05, random_state=42)\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f3c1a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11870' max='11870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11870/11870 1:39:29, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.028100</td>\n",
       "      <td>3.997821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.913500</td>\n",
       "      <td>3.934950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.864000</td>\n",
       "      <td>3.901535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.815800</td>\n",
       "      <td>3.885563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.794800</td>\n",
       "      <td>3.880372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11870, training_loss=3.911005846391532, metrics={'train_runtime': 5970.6407, 'train_samples_per_second': 63.612, 'train_steps_per_second': 1.988, 'total_flos': 2.481000505344e+16, 'train_loss': 3.911005846391532, 'epoch': 5.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7bd1df",
   "metadata": {},
   "source": [
    "## SAVE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6188d353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./gpt2-extended-data-poem-model/tokenizer_config.json',\n",
       " './gpt2-extended-data-poem-model/special_tokens_map.json',\n",
       " './gpt2-extended-data-poem-model/vocab.json',\n",
       " './gpt2-extended-data-poem-model/merges.txt',\n",
       " './gpt2-extended-data-poem-model/added_tokens.json',\n",
       " './gpt2-extended-data-poem-model/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./gpt2-extended-data-poem-model\")\n",
    "tokenizer.save_pretrained(\"./gpt2-extended-data-poem-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ae5c1d",
   "metadata": {},
   "source": [
    "## GENERATE A POEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b18b7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It never ends, nor do I go,\n",
      "I have nothing but grief and pain,\n",
      "That comes to me through the night —\n",
      "With my dead and sorrow, and a ghost\n",
      "That calls me to his, and I am alone\n",
      "To him I love.\n",
      "I would never have seen a thing\n",
      "So fair and so beautiful,\n",
      "How beautiful would I have loved them all,\n",
      "How beautiful for a mother\n",
      "To see such a child come to her.\n",
      "I would have passed\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"./gpt2-extended-data-poem-model\", tokenizer=\"./gpt2-extended-data-poem-model\")\n",
    "\n",
    "prompt = \"It never ends\"\n",
    "results = generator(prompt, max_length=100, num_return_sequences=1, do_sample=True, top_k=50, top_p=0.95)\n",
    "\n",
    "print(results[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05f7aec",
   "metadata": {},
   "source": [
    "## EVALUATION METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe0a7e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "prompts = [\n",
    "    \"It never ends\",\n",
    "    \"The moonlight dances\",\n",
    "    \"Darkness falls quickly\",\n",
    "    \"Beneath the willow tree\",\n",
    "    \"Whispers in the wind\",\n",
    "    \"I dreamed of fire\",\n",
    "    \"The silence grew louder\",\n",
    "    \"Stars fell like rain\",\n",
    "    \"Time forgets no one\",\n",
    "    \"A rose in winter\",\n",
    "    \"Shadows crawl at dawn\",\n",
    "    \"My heart is a lantern\",\n",
    "    \"Echoes of your name\",\n",
    "    \"Frozen in memory\",\n",
    "    \"We walked on glass\",\n",
    "    \"The sky swallowed the sun\",\n",
    "    \"Love fades to smoke\",\n",
    "    \"Buried beneath the snow\",\n",
    "    \"A storm without sound\",\n",
    "    \"Hope wears thin threads\"\n",
    "]\n",
    "\n",
    "all_poems = df[\"text\"].tolist()\n",
    "\n",
    "poem_embeddings = model.encode(all_poems, convert_to_tensor=True)\n",
    "\n",
    "best_refs = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    prompt_embedding = model.encode(prompt, convert_to_tensor=True)\n",
    "    similarities = util.cos_sim(prompt_embedding, poem_embeddings)[0]\n",
    "    best_index = similarities.argmax().item()\n",
    "    best_poem = all_poems[best_index]\n",
    "    best_refs.append(best_poem)\n",
    "    #print(f\"\\nPrompt: {prompt}\\nBest Reference Poem:\\n{best_poem}\\n{'-'*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a6f935c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores:\n",
      "rouge1: 0.2112\n",
      "rouge2: 0.0202\n",
      "rougeL: 0.1396\n",
      "rougeLsum: 0.2064\n",
      "BERTScore (averaged):\n",
      "Precision: 0.8319\n",
      "Recall: 0.8302\n",
      "F1: 0.8309\n"
     ]
    }
   ],
   "source": [
    "#generator = pipeline(\"text-generation\", model=\"./gpt2-extended-data-poem-model\", tokenizer=\"./gpt2-extended-data-poem-model\")\n",
    "\n",
    "generated_poems = [\n",
    "    generator(prompt, max_length=100, num_return_sequences=1, do_sample=True, top_k=50, top_p=0.95)[0][\"generated_text\"]\n",
    "    for prompt in prompts\n",
    "]\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "rouge_score = rouge.compute(predictions=generated_poems, references=best_refs)\n",
    "bert_score = bertscore.compute(predictions=generated_poems, references=best_refs, lang=\"en\")\n",
    "\n",
    "print(\"ROUGE Scores:\")\n",
    "for key, val in rouge_score.items():\n",
    "    print(f\"{key}: {round(val, 4)}\")\n",
    "\n",
    "print(\"BERTScore (averaged):\")\n",
    "print(\"Precision:\", round(sum(bert_score[\"precision\"]) / len(bert_score[\"precision\"]), 4))\n",
    "print(\"Recall:\", round(sum(bert_score[\"recall\"]) / len(bert_score[\"recall\"]), 4))\n",
    "print(\"F1:\", round(sum(bert_score[\"f1\"]) / len(bert_score[\"f1\"]), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69ae19a",
   "metadata": {},
   "source": [
    "## METRICS ANALYSIS COMPARED TO THE OTHER NOTEBOOK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250c9f9b",
   "metadata": {},
   "source": [
    "ROUGE-1:\n",
    "\n",
    "We have a slightly better ROUGE score results, which suggests that the model is producing outputs that have more literal word overlap with references. Possibly, the added data increased lexical diversity or bias toward more common words.\n",
    "\n",
    "BERTScore F1:\n",
    "\n",
    "We have slightly less worse scores when it comes to BERT, which suggests that our model's semantic alignment with the reference may have slightly weakened. It could be due to the fact that the style or vocabulary of the new translated poems differs significantly from the original dataset, or that the translations introduced syntactic artifacts or inconsistencies, impacting fluency or meaning.\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "Adding the translated poems increased lexical coverage or surface-level similarity (ROUGE-1), but might have introduced slight semantic drift (BERTScore F1) — possibly due to style, translation artifacts, or inconsistencies in tone or language quality.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
